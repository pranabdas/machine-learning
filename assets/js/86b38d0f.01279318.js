"use strict";(self.webpackChunkmachine_learning=self.webpackChunkmachine_learning||[]).push([[967],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return m}});var r=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=r.createContext({}),c=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},u=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},f=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),f=c(n),m=o,g=f["".concat(l,".").concat(m)]||f[m]||p[m]||i;return n?r.createElement(g,a(a({ref:t},u),{},{components:n})):r.createElement(g,a({ref:t},u))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,a=new Array(i);a[0]=f;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,a[1]=s;for(var c=2;c<i;c++)a[c]=n[c];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}f.displayName="MDXCreateElement"},216:function(e,t,n){n.r(t),n.d(t,{assets:function(){return u},contentTitle:function(){return l},default:function(){return m},frontMatter:function(){return s},metadata:function(){return c},toc:function(){return p}});var r=n(7462),o=n(3366),i=(n(7294),n(3905)),a=["components"],s={title:"Boosting"},l=void 0,c={unversionedId:"boosting",id:"boosting",title:"Boosting",description:"Boosting is an ensemble classifier, meaning it combines multiple algorithms.",source:"@site/docs/boosting.md",sourceDirName:".",slug:"/boosting",permalink:"/machine-learning/boosting",draft:!1,editUrl:"https://github.com/pranabdas/machine-learning/blob/main/docs/boosting.md",tags:[],version:"current",frontMatter:{title:"Boosting"},sidebar:"docs",previous:{title:"Random Forrest",permalink:"/machine-learning/random-forrest"},next:{title:"Resources",permalink:"/machine-learning/resources"}},u={},p=[{value:"AdaBoost (Adaptive Boosting)",id:"adaboost",level:3},{value:"Resources",id:"resources",level:3}],f={toc:p};function m(e){var t=e.components,n=(0,o.Z)(e,a);return(0,i.kt)("wrapper",(0,r.Z)({},f,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Boosting is an ensemble classifier, meaning it combines multiple algorithms.\nBoosting can be used to improve the performance of weak learner. If we have an\nalgorithm that is better than random but performs poorly by itself. In such\nsituations, boosting can be used to improve its performance. During training,\nboosting goes through the dataset several times. We use our actual algorithm,\nand look into its results. If the algorithm predicts correct result for a data\npoint, we leave the weight of that data points as it is. However, if the\nalgorithm predicts wrong, we increase the weight of those data points (a weight\nof 2, means having that data point twice) and apply our algorithm again. Most\nlikely, we will obtain different prediction output. We repeat the process for\ncertain number of times. Our final boosting algorithm is a weighted (this weight\nis the ratio of how well a specific boosting performed) sum of all these\nindividual boosting. Overall this can improve the performance of a weak learning\nalgorithm drastically."),(0,i.kt)("h3",{id:"adaboost"},"AdaBoost (Adaptive Boosting)"),(0,i.kt)("h3",{id:"resources"},"Resources"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://api.semanticscholar.org/CorpusID:1836349"},"Original AdaBoost paper"))))}m.isMDXComponent=!0}}]);