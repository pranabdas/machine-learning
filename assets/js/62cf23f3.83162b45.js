"use strict";(self.webpackChunkmachine_learning=self.webpackChunkmachine_learning||[]).push([[673],{3905:function(e,r,n){n.d(r,{Zo:function(){return u},kt:function(){return p}});var t=n(7294);function o(e,r,n){return r in e?Object.defineProperty(e,r,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[r]=n,e}function a(e,r){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);r&&(t=t.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),n.push.apply(n,t)}return n}function i(e){for(var r=1;r<arguments.length;r++){var n=null!=arguments[r]?arguments[r]:{};r%2?a(Object(n),!0).forEach((function(r){o(e,r,n[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(n,r))}))}return e}function s(e,r){if(null==e)return{};var n,t,o=function(e,r){if(null==e)return{};var n,t,o={},a=Object.keys(e);for(t=0;t<a.length;t++)n=a[t],r.indexOf(n)>=0||(o[n]=e[n]);return o}(e,r);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(t=0;t<a.length;t++)n=a[t],r.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var c=t.createContext({}),l=function(e){var r=t.useContext(c),n=r;return e&&(n="function"==typeof e?e(r):i(i({},r),e)),n},u=function(e){var r=l(e.components);return t.createElement(c.Provider,{value:r},e.children)},m="mdxType",f={inlineCode:"code",wrapper:function(e){var r=e.children;return t.createElement(t.Fragment,{},r)}},d=t.forwardRef((function(e,r){var n=e.components,o=e.mdxType,a=e.originalType,c=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),m=l(n),d=o,p=m["".concat(c,".").concat(d)]||m[d]||f[d]||a;return n?t.createElement(p,i(i({ref:r},u),{},{components:n})):t.createElement(p,i({ref:r},u))}));function p(e,r){var n=arguments,o=r&&r.mdxType;if("string"==typeof e||o){var a=n.length,i=new Array(a);i[0]=d;var s={};for(var c in r)hasOwnProperty.call(r,c)&&(s[c]=r[c]);s.originalType=e,s[m]="string"==typeof e?e:o,i[1]=s;for(var l=2;l<a;l++)i[l]=n[l];return t.createElement.apply(null,i)}return t.createElement.apply(null,n)}d.displayName="MDXCreateElement"},5710:function(e,r,n){n.r(r),n.d(r,{assets:function(){return u},contentTitle:function(){return c},default:function(){return d},frontMatter:function(){return s},metadata:function(){return l},toc:function(){return m}});var t=n(7462),o=n(3366),a=(n(7294),n(3905)),i=["components"],s={title:"Random Forrest"},c=void 0,l={unversionedId:"random-forrest",id:"random-forrest",title:"Random Forrest",description:"Random Forrest is an ensemble classifier (it can also be used as a regressor,",source:"@site/docs/random-forrest.md",sourceDirName:".",slug:"/random-forrest",permalink:"/machine-learning/random-forrest",draft:!1,editUrl:"https://github.com/pranabdas/machine-learning/blob/main/docs/random-forrest.md",tags:[],version:"current",frontMatter:{title:"Random Forrest"},sidebar:"docs",previous:{title:"Hierarchical Clustering",permalink:"/machine-learning/hierarchical-clustering"},next:{title:"Boosting",permalink:"/machine-learning/boosting"}},u={},m=[],f={toc:m};function d(e){var r=e.components,n=(0,o.Z)(e,i);return(0,a.kt)("wrapper",(0,t.Z)({},f,n,{components:r,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"Random Forrest is an ensemble classifier (it can also be used as a regressor,\ninstead of majority voting we consider the weighted average of each individual\ntree for the final outcome). It combines multiple decision tree classifiers to\nform a final single classifier. While decision trees are common and simple\nsupervised learning method, they have some drawbacks as well, such as bias and\noverfitting. Such drawbacks can be overcome by using multiple decision trees to\nform an ensemble in the random forest algorithm. They improve the predictions,\nparticularly when the individual trees are uncorrelated (random) with each\nother."),(0,a.kt)("p",null,"Unlike a single decision tree, random forrest selects a subset of features as\nwell. This can reduce the risk of overfitting. Each individual tree predicts a\noutput class, and the class gets majority voting is the model's prediction.\nRandom forest algorithms have three main hyperparameters: node size, the number\nof trees, and the number of features sampled."))}d.isMDXComponent=!0}}]);