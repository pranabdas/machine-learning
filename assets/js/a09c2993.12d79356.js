"use strict";(self.webpackChunkmachine_learning=self.webpackChunkmachine_learning||[]).push([[128],{3905:function(e,n,t){t.d(n,{Zo:function(){return p},kt:function(){return d}});var r=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=r.createContext({}),c=function(e){var n=r.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},p=function(e){var n=c(e.components);return r.createElement(s.Provider,{value:n},e.children)},m={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},u=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=c(t),d=a,g=u["".concat(s,".").concat(d)]||u[d]||m[d]||i;return t?r.createElement(g,o(o({ref:n},p),{},{components:t})):r.createElement(g,o({ref:n},p))}));function d(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,o=new Array(i);o[0]=u;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l.mdxType="string"==typeof e?e:a,o[1]=l;for(var c=2;c<i;c++)o[c]=t[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}u.displayName="MDXCreateElement"},8495:function(e,n,t){t.r(n),t.d(n,{assets:function(){return p},contentTitle:function(){return s},default:function(){return d},frontMatter:function(){return l},metadata:function(){return c},toc:function(){return m}});var r=t(7462),a=t(3366),i=(t(7294),t(3905)),o=["components"],l={title:"Introduction to machine learning",sidebar_label:"Introduction",slug:"/",keywords:["machine learning","data science","generative model","linear regression","logistic regression","nearest neighbor","ridge regression","optimization","k-means","hierarchical clustering","support vector machines","principal component analysis","random forrest","boosting"]},s=void 0,c={unversionedId:"introduction",id:"introduction",title:"Introduction to machine learning",description:"There are certain type of problems we face in everyday life that cannot be",source:"@site/docs/introduction.md",sourceDirName:".",slug:"/",permalink:"/machine-learning/",draft:!1,editUrl:"https://github.com/pranabdas/machine-learning/blob/main/docs/introduction.md",tags:[],version:"current",frontMatter:{title:"Introduction to machine learning",sidebar_label:"Introduction",slug:"/",keywords:["machine learning","data science","generative model","linear regression","logistic regression","nearest neighbor","ridge regression","optimization","k-means","hierarchical clustering","support vector machines","principal component analysis","random forrest","boosting"]},sidebar:"docs",next:{title:"Computing setup",permalink:"/machine-learning/setup"}},p={},m=[{value:"The Machine Learning Process",id:"the-machine-learning-process",level:3},{value:"Machine learning algorithms",id:"machine-learning-algorithms",level:3}],u={toc:m};function d(e){var n=e.components,t=(0,a.Z)(e,o);return(0,i.kt)("wrapper",(0,r.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"There are certain type of problems we face in everyday life that cannot be\nsolved by definitive algorithm, e.g., object recognition, predicting financial\nmarkets etc. In such cases, we collect a set of input and output samples (called\ntraining set); and based on the trends on those data, we try to predict future\noutcomes."),(0,i.kt)("p",null,"We will see there are three broad category of machine learning problems: (1)\nsupervised, (2) unsupervised learning, and (3) reinforcement learning. As the\nnames suggest in the first case, we have data available to train our model."),(0,i.kt)("p",null,"(1) Supervised machine learning problems has two categories (a) classification\nand (b) regression. In case of classification, we have discrete targets/labels (\ne.g., whether an email is real or spam, whether the image is of a cat or dog),\nwhile in case of regression, we need to predict a quantitative value (e.g.,\npredicting second hand car price based on a set of features like brand, color,\nmileage driven, engine size etc.). The supervised regression is more like\nstandard fitting program."),(0,i.kt)("p",null,"(2) On the other hand, in case of unsupervised learning problem, we do not have\nassociated data labels. Like segmenting customers for a supermarket or bank,\nclustering similar music or movies etc."),(0,i.kt)("p",null,"(3) Finally, reinforcement learning is based on the interaction, it finds the\noptimal solution for a dataset in order to maximize a reward."),(0,i.kt)("h3",{id:"the-machine-learning-process"},"The Machine Learning Process"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Data collection and preparation"),(0,i.kt)("li",{parentName:"ol"},"Feature selection"),(0,i.kt)("li",{parentName:"ol"},"Algorithm selection"),(0,i.kt)("li",{parentName:"ol"},"Parameter and Model selection"),(0,i.kt)("li",{parentName:"ol"},"Training"),(0,i.kt)("li",{parentName:"ol"},"Evaluation")),(0,i.kt)("h3",{id:"machine-learning-algorithms"},"Machine learning algorithms"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Supervised learning:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Decision trees"),(0,i.kt)("li",{parentName:"ul"},"Naive Bayes"),(0,i.kt)("li",{parentName:"ul"},"Least squares regression"),(0,i.kt)("li",{parentName:"ul"},"Logistic regression"),(0,i.kt)("li",{parentName:"ul"},"Neural Networks"),(0,i.kt)("li",{parentName:"ul"},"Support Vector Machines"),(0,i.kt)("li",{parentName:"ul"},"Ensemble methods"))),(0,i.kt)("li",{parentName:"ul"},"Unsupervised learning:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Clustering"),(0,i.kt)("li",{parentName:"ul"},"Principal Component Analysis"),(0,i.kt)("li",{parentName:"ul"},"Singular Value Decomposition"),(0,i.kt)("li",{parentName:"ul"},"Independent Component Analysis"))),(0,i.kt)("li",{parentName:"ul"},"Reinforcement learning:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Q Learning"),(0,i.kt)("li",{parentName:"ul"},"T Learning"),(0,i.kt)("li",{parentName:"ul"},"Adversarial Learning"),(0,i.kt)("li",{parentName:"ul"},"Genetic Algorithms")))),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"Question?")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"Today data science is widely utilized by technology companies to deliver\nadvertisement recommendations, other recommendation systems like news feed,\nsongs, videos, shopping; supply and demand analysis, speech recognition/ natural\nlanguage processing, lifestyle improvements including medical applications,\nrecognize pattern to detect frauds etc. Most of these applications requires\nextremely large and complex datasets. Are there applications where small\ndatasets from our day to day life can be useful in a practical way?"))))}d.isMDXComponent=!0}}]);